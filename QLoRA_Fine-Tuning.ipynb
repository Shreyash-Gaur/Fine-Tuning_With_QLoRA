{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7hYqUsxYQtl"
   },
   "source": [
    "# (QLora) Fine-tuning Mistral-7b-Instruct to Respond to YouTube Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT-8d3vZYG4W"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tfSl5xRs0y8J"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVstnrh-0m2x"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFcKal6c96su",
    "outputId": "a32ee85e-a2ac-4784-9665-e16238c67208"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
    "                                             trust_remote_code=False, # prevents running custom model files on your machine\n",
    "                                             revision=\"main\") # which version of model to use in repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCKC3_yl0pNS"
   },
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c3CIa8C80Vtm"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnFiyBOc329l"
   },
   "source": [
    "## Using Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Du9KLy3N8Ap6",
    "outputId": "f2366af3-1c89-4664-f0ae-21614f735580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Great content, thank you! [/INST] I'm glad you found the content helpful! If you have any specific questions or topics you'd like me to cover in the future, feel free to ask. I'm here to help.\n",
      "\n",
      "In the meantime, I'd be happy to answer any questions you have about the content I've already provided. Just let me know which article or blog post you're referring to, and I'll do my best to provide you with accurate and up-to-date information.\n",
      "\n",
      "Thanks for reading, and I look forward to helping you with any questions you may have!</s>\n"
     ]
    }
   ],
   "source": [
    "model.eval() # model in evaluation mode (dropout modules are deactivated)\n",
    "\n",
    "# craft prompt\n",
    "comment = \"Great content, thank you!\"\n",
    "prompt=f'''[INST] {comment} [/INST]'''\n",
    "\n",
    "# tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate output\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7XAi8rtC5a9"
   },
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U77JPibZ4QuK",
    "outputId": "d7e000ac-93c6-4cda-c76a-83cb0fa34433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "Great content, thank you! \n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "intstructions_string = f\"\"\"ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. \\\n",
    "ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
    "\n",
    "prompt = prompt_template(comment)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUaf1YyZ7R7G",
    "outputId": "d9a439bb-878a-4a8a-ec20-041c40e4e74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "Great content, thank you! \n",
      "[/INST] Thank you for your kind words! I'm glad you found the content helpful. –ShreyashGPT</s>\n"
     ]
    }
   ],
   "source": [
    "# tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate output\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXc8pBsVIy2N"
   },
   "source": [
    "## Prepare Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K3PesDIyI1Hn"
   },
   "outputs": [],
   "source": [
    "model.train() # model in training mode (dropout modules are activated)\n",
    "\n",
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXOHXZcII5FS",
    "outputId": "4e429a59-054b-471e-daaa-65e5ac7f1f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n"
     ]
    }
   ],
   "source": [
    "# LoRA config\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['example'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory where the dataset is saved\n",
    "output_dir = r\"D:\\Projects\\Projects\\LLM\\Fine-Tuning\\Fine-Tuning using QLoRA\\data\"\n",
    "\n",
    "# Load the dataset\n",
    "data = load_from_disk(output_dir)\n",
    "\n",
    "# Verify that the dataset is loaded\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tssCKXx3hUqM"
   },
   "source": [
    "## Preparing Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D9LEA_g9gZPF"
   },
   "outputs": [],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"example\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pow8yLjKcJM8"
   },
   "outputs": [],
   "source": [
    "# setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-evbaTxhQTC"
   },
   "source": [
    "## Fine-tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e1PPLr4McNii"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "# define training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir= \"model/\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "m6wZK62bJKsJ",
    "outputId": "d50932ac-ac76-418e-e8dd-dc7db2a3dc75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4215, 'learning_rate': 0.00019285714285714286, 'epoch': 0.92}\n",
      "{'eval_loss': 4.04224157333374, 'eval_runtime': 5.6995, 'eval_samples_per_second': 1.579, 'eval_steps_per_second': 0.526, 'epoch': 0.92}\n",
      "{'loss': 3.9183, 'learning_rate': 0.00017142857142857143, 'epoch': 1.85}\n",
      "{'eval_loss': 3.4363341331481934, 'eval_runtime': 5.7331, 'eval_samples_per_second': 1.57, 'eval_steps_per_second': 0.523, 'epoch': 1.85}\n",
      "{'loss': 3.3404, 'learning_rate': 0.00015000000000000001, 'epoch': 2.77}\n",
      "{'eval_loss': 2.931706190109253, 'eval_runtime': 5.7552, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.521, 'epoch': 2.77}\n",
      "{'loss': 2.1604, 'learning_rate': 0.00012142857142857143, 'epoch': 4.0}\n",
      "{'eval_loss': 2.414346933364868, 'eval_runtime': 5.1835, 'eval_samples_per_second': 1.736, 'eval_steps_per_second': 0.579, 'epoch': 4.0}\n",
      "{'loss': 2.5228, 'learning_rate': 0.0001, 'epoch': 4.92}\n",
      "{'eval_loss': 2.063117265701294, 'eval_runtime': 5.6972, 'eval_samples_per_second': 1.58, 'eval_steps_per_second': 0.527, 'epoch': 4.92}\n",
      "{'loss': 2.2259, 'learning_rate': 7.857142857142858e-05, 'epoch': 5.85}\n",
      "{'eval_loss': 1.78472900390625, 'eval_runtime': 5.4803, 'eval_samples_per_second': 1.642, 'eval_steps_per_second': 0.547, 'epoch': 5.85}\n",
      "{'loss': 1.98, 'learning_rate': 5.714285714285714e-05, 'epoch': 6.77}\n",
      "{'eval_loss': 1.5560029745101929, 'eval_runtime': 5.7288, 'eval_samples_per_second': 1.571, 'eval_steps_per_second': 0.524, 'epoch': 6.77}\n",
      "{'loss': 1.3615, 'learning_rate': 2.857142857142857e-05, 'epoch': 8.0}\n",
      "{'eval_loss': 1.403984785079956, 'eval_runtime': 5.2473, 'eval_samples_per_second': 1.715, 'eval_steps_per_second': 0.572, 'epoch': 8.0}\n",
      "{'loss': 1.7342, 'learning_rate': 7.142857142857143e-06, 'epoch': 8.92}\n",
      "{'eval_loss': 1.3528900146484375, 'eval_runtime': 5.7792, 'eval_samples_per_second': 1.557, 'eval_steps_per_second': 0.519, 'epoch': 8.92}\n",
      "{'loss': 1.1748, 'learning_rate': 0.0, 'epoch': 9.23}\n",
      "{'eval_loss': 1.347071886062622, 'eval_runtime': 5.1808, 'eval_samples_per_second': 1.737, 'eval_steps_per_second': 0.579, 'epoch': 9.23}\n",
      "{'train_runtime': 4217.1868, 'train_samples_per_second': 0.119, 'train_steps_per_second': 0.007, 'train_loss': 2.523072330156962, 'epoch': 9.23}\n"
     ]
    }
   ],
   "source": [
    "# configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "# train model\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3-3yynACoX0"
   },
   "source": [
    "## Push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331,
     "referenced_widgets": [
      "04080836c74c4420aad4d3d0e3a07201",
      "0c69ab0d74da42a0af049615bc4a2a05",
      "664619de2e82400681039651e04180e7",
      "dd64516748874487bae40e90a2b9bf9b",
      "9484e30b05e740df9b5c63dc372cfccf",
      "08c7b27499614bf4b5acfa3853a0bfe1",
      "6ed0223f808742f9bb16280510b04984",
      "985fe3322676407d9833f947070f3336",
      "3963c9887c864441aa79be69d6d3cdd2",
      "01b63497cb0c4c3cb96a9a3239f38f79",
      "902d5209dc7f4610914c5d74df5e3b25",
      "938a8323cc5143baa776331e0b127465",
      "834b0b4ea37d4d98ba5db09548dfa75f",
      "c37c28d96a11422ba7c29b9ca730d00d",
      "59851cd896fd416d8298a093650e655e",
      "117894e524414d2a806df21633dcf679",
      "84d329350a48481b91828e975ed41f5d"
     ]
    },
    "id": "_c5KhuhpCno-",
    "outputId": "e7394aab-d039-4d3a-e3e3-d4165a7943c9"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# # option 2: key login\n",
    "# from huggingface_hub import login\n",
    "# write_key = 'hf_' # paste token here\n",
    "# login(write_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYLbvvuPCq7q"
   },
   "outputs": [],
   "source": [
    "hf_name = 'Shreyash01' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + \"Shreyash_yt_Gpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "c4nAFjYPDC4_",
    "outputId": "e9b6d4ca-a3f8-42cc-bd9c-2d98a5feccc6"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(model_id)\n",
    "trainer.push_to_hub(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDxRMA_BCBAt"
   },
   "source": [
    "## Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9gRgNjNiTvK"
   },
   "outputs": [],
   "source": [
    "# # load model from hub\n",
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# config = PeftConfig.from_pretrained(\"Shreyash01/Shreyash_yt_Gpt\")\n",
    "# model = PeftModel.from_pretrained(model, \"Shreyash01/Shreyash_yt_Gpt\")\n",
    "\n",
    "# # load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJIo4nUnhgiU"
   },
   "source": [
    "## Use Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ITUJZDIYjs4t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "Great content, thank you! \n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "intstructions_string = f\"\"\"ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. \\\n",
    "ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
    "\n",
    "comment = \"Great content, thank you!\"\n",
    "\n",
    "prompt = prompt_template(comment)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "j9wNZ2URivBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "Great content, thank you! \n",
      "[/INST]\n",
      "* [ShreyashGPT]\n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "p1Pzx5q_wt2z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] ShreyashGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShreyashGPT'. ShreyashGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "What is fat-tailedness? \n",
      "[/INST]\n",
      "\n",
      "Great question!\n",
      "\n",
      "Fat-tailedness is a statistical property where the distribution of data has heavier tails than a normal distribution. In simpler terms, it means that extreme values occur more frequently than what a normal distribution would predict.\n",
      "\n",
      "For example, in finance, stock prices follow a fat-tailed distribution. While the majority of returns are close to zero, extreme returns (both positive and negative) occur more frequently than what a normal distribution would suggest.\n",
      "\n",
      "–ShreyashGPT</s>\n"
     ]
    }
   ],
   "source": [
    "comment = \"What is fat-tailedness?\"\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "model.eval()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OybvrnR5Qy_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01b63497cb0c4c3cb96a9a3239f38f79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04080836c74c4420aad4d3d0e3a07201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c69ab0d74da42a0af049615bc4a2a05",
       "IPY_MODEL_664619de2e82400681039651e04180e7",
       "IPY_MODEL_dd64516748874487bae40e90a2b9bf9b",
       "IPY_MODEL_9484e30b05e740df9b5c63dc372cfccf",
       "IPY_MODEL_08c7b27499614bf4b5acfa3853a0bfe1"
      ],
      "layout": "IPY_MODEL_6ed0223f808742f9bb16280510b04984"
     }
    },
    "08c7b27499614bf4b5acfa3853a0bfe1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_117894e524414d2a806df21633dcf679",
      "placeholder": "​",
      "style": "IPY_MODEL_84d329350a48481b91828e975ed41f5d",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "0c69ab0d74da42a0af049615bc4a2a05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_985fe3322676407d9833f947070f3336",
      "placeholder": "​",
      "style": "IPY_MODEL_3963c9887c864441aa79be69d6d3cdd2",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "117894e524414d2a806df21633dcf679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3963c9887c864441aa79be69d6d3cdd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59851cd896fd416d8298a093650e655e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "664619de2e82400681039651e04180e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_01b63497cb0c4c3cb96a9a3239f38f79",
      "placeholder": "​",
      "style": "IPY_MODEL_902d5209dc7f4610914c5d74df5e3b25",
      "value": ""
     }
    },
    "6ed0223f808742f9bb16280510b04984": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "834b0b4ea37d4d98ba5db09548dfa75f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84d329350a48481b91828e975ed41f5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "902d5209dc7f4610914c5d74df5e3b25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "938a8323cc5143baa776331e0b127465": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9484e30b05e740df9b5c63dc372cfccf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_c37c28d96a11422ba7c29b9ca730d00d",
      "style": "IPY_MODEL_59851cd896fd416d8298a093650e655e",
      "tooltip": ""
     }
    },
    "985fe3322676407d9833f947070f3336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c37c28d96a11422ba7c29b9ca730d00d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd64516748874487bae40e90a2b9bf9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_938a8323cc5143baa776331e0b127465",
      "style": "IPY_MODEL_834b0b4ea37d4d98ba5db09548dfa75f",
      "value": true
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
